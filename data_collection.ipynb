{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://en.wikipedia.org/wiki/List_of_sculptors\"\n",
    "\n",
    "def get_sculptors(url, limit=100):\n",
    "    # Get the page\n",
    "    response = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    sculptors = {}\n",
    "    list_elements = soup.find('div', class_='div-col')\n",
    "    # Get the list of sculptors\n",
    "    if list_elements:\n",
    "        list_sculptors = list_elements.find_all('li')\n",
    "    \n",
    "        for idx, sculptor in enumerate(list_sculptors):\n",
    "            if idx == limit:\n",
    "                break\n",
    "            link = sculptor.find('a')\n",
    "            # Get the name and the link\n",
    "            if link:\n",
    "                name = link.text.strip()\n",
    "                sculptor = link.get('href')\n",
    "                full_url = f\"https://en.wikipedia.org{sculptor}\"\n",
    "                sculptors[name] = full_url\n",
    "\n",
    "    return sculptors\n",
    "\n",
    "\n",
    "sculptors = get_sculptors(BASE_URL)\n",
    "\n",
    "# Save the names to a file for later use\n",
    "with open(\"names.txt\", \"w\") as file:\n",
    "  for i in sculptors:\n",
    "    file.write(str(i) + \"\\n\")\n",
    "\n",
    "# print(sculptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(title):\n",
    "    # Define the base URL for the Wikipedia API\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Define the parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": title,\n",
    "        \"explaintext\": True,  # Return plain text instead of HTML\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the Wikipedia API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the page content from the API response\n",
    "    try:\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        if pages:\n",
    "            # Retrieve the content of the first page (there should be only one page)\n",
    "            page_id = list(pages.keys())[0]\n",
    "            content = pages[page_id].get(\"extract\", \"\")\n",
    "            return content\n",
    "        else:\n",
    "            return None\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"DisambiguationError for : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Sculptors'):\n",
    "    os.makedirs('data/Sculptors')\n",
    "data_sculp = []\n",
    "for name, link in sculptors.items():\n",
    "    content  = get_summaries(name)\n",
    "    if content:\n",
    "        # Append data to list\n",
    "        data_sculp.append({'Category': 'Sculptors', 'Name': name, 'Content': content})\n",
    "        filename = f'Sculptors/{name}_sculptor.txt'\n",
    "        # Save the content to a text file\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "biography = pd.DataFrame(data_sculp)\n",
    "biography.to_csv('data/biography_sculptor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_computer_scientists(url, limit=100):\n",
    "    response = requests.get(url=url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    computer_scientists = {}\n",
    "    list_elements = soup.find('div', class_='mw-body-content')\n",
    "    if list_elements:\n",
    "        list_computer_scientists = list_elements.find_all('li')\n",
    "        for idx, computer_scientist in enumerate(list_computer_scientists):\n",
    "            if idx >= limit:\n",
    "                break\n",
    "            link = computer_scientist.find('a')\n",
    "            if link:\n",
    "                name = link.text.strip()\n",
    "                computer_scientist = link.get('href')\n",
    "                full_url = f\"https://en.wikipedia.org{computer_scientist}\"\n",
    "                computer_scientists[name] = full_url\n",
    "\n",
    "    return computer_scientists\n",
    "\n",
    "computers = get_computer_scientists('https://en.wikipedia.org/wiki/List_of_computer_scientists')\n",
    "# sculptors = get_sculptors(BASE_URL)\n",
    "# with open(\"names.txt\", \"a\") as file:\n",
    "#   for i in computers:\n",
    "#     file.write(str(i) + \"\\n\")\n",
    "# print(computers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Computer_Scientists'):\n",
    "    os.makedirs('data/Computer_Scientists')\n",
    "data_cs = []\n",
    "for name, link in computers.items():\n",
    "    content  = get_summaries(name)\n",
    "    if content:\n",
    "        # Append data to list\n",
    "        data_cs.append({'Category': 'Computer_Scientists', 'Name': name, 'Content': content})\n",
    "        filename = f'Computer_Scientists/{name}_computerscientist.txt'\n",
    "        # Save the content to a text file\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "biography_cs = pd.DataFrame(data_cs)\n",
    "biography_cs.to_csv('data/biography_cs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch RDF triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdf_triples(name):\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    encoded_name = name.replace(' ', '_')\n",
    "   \n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT ?subject ?predicate ?object\n",
    "    WHERE {{\n",
    "      {{\n",
    "        ?subject ?predicate dbr:{encoded_name} .\n",
    "        ?subject ?predicate ?object .\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        ?object ?predicate dbr:{encoded_name} .\n",
    "        ?subject ?predicate ?object .\n",
    "      }}\n",
    "    }}\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    triples = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        predicate = result[\"predicate\"][\"value\"]\n",
    "        obj = result[\"object\"][\"value\"]\n",
    "        triples.append({'subject': f\"http://dbpedia.org/resource/{encoded_name}\", 'predicate': predicate, 'object': obj})\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rdf_triples(names, output_dir):\n",
    "    category_dir = os.path.join(output_dir)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "    for name in names.keys():\n",
    "        try:\n",
    "            triples = get_rdf_triples(name)\n",
    "\n",
    "            if triples:\n",
    "                output_file = os.path.join(category_dir, f\"{name}.json\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(triples, f, indent=4)\n",
    "            else:\n",
    "                print(f\"No RDF triples found for {name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rdf_triples(sculptors, 'sculptors_rdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_rdf_triples(computers, 'computers_rdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
