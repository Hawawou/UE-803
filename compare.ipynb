{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0fb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/oumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/oumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries (will need to export to requirements.txt)\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import wikipedia\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import collections\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# import glob\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull 100 biographies from wikipedia for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5f22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "CATEGORY1 = 'Sculptors'\n",
    "CATEGORY2 = 'Computer_scientists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5bbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_people(profession):\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    \n",
    "    modified_profession = profession.replace(' ', '_').replace(\"'\", '%27')\n",
    "    \n",
    "    query = \"\"\"\n",
    "            Select Distinct ?person Where {\n",
    "                ?person dbo:occupation dbr:%s .\n",
    "            }\n",
    "            \"\"\" % modified_profession\n",
    "    \n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    ret = sparql.queryAndConvert()\n",
    "    return(ret['results']['bindings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed78466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdf_triples(person):\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    dbpedia_resource = \"http://dbpedia.org/resource/\" + person\n",
    "    query = \"SELECT ?predicate ?object WHERE { <\" + dbpedia_resource + \"> ?predicate ?object.}\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    triples = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        subj = \"<\"+dbpedia_resource+\">\"\n",
    "        pred = result[\"predicate\"][\"value\"]\n",
    "        obj = result[\"object\"][\"value\"]\n",
    "        triples.append({'subject': subj, 'predicate': pred, 'object': obj})\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadadc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biographies(people, category, limit):\n",
    "    pages = []\n",
    "    i = 0\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print('Data Directory did not exist, creating it now...')\n",
    "        os.mkdir(DATA_PATH)\n",
    "\n",
    "    with tqdm(total=limit) as pbar:\n",
    "        while len(pages) < limit:\n",
    "            if i >= len(people):\n",
    "                break\n",
    "            person = people[i]['person']['value'].split('/')[-1]\n",
    "            try:\n",
    "                page = wikipedia.page(person).content\n",
    "                pages.append(page)\n",
    "                file_path = os.path.join(DATA_PATH, person + '_' + category.replace(' ', '_'))\n",
    "                with open(file_path + '.txt', 'w') as f:\n",
    "                    f.write(page)\n",
    "                triples = get_rdf_triples(person)\n",
    "                with open(file_path + '.json', 'w') as f:\n",
    "                    json.dump(triples, f, indent=4)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            i += 1\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c8592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(cat1, cat2, limit=999):\n",
    "    people1, people2 = (get_people(cat1), get_people(cat2))\n",
    "    texts1, texts2 = (get_biographies(people1, cat1, limit), get_biographies(people2, cat2, limit))\n",
    "    df = pd.DataFrame({'text' : texts1+texts2})\n",
    "    df['category'] = pd.Series([cat1]*len(texts1) + [cat2]*len(texts2))\n",
    "    return df, texts1, texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8fac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:01<01:48,  1.10s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "df, texts1, texts2 = create_dataset(CATEGORY1, CATEGORY2, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a56373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_complete(text):\n",
    "    dirty_chars = '{}[]()|\\/=+`\"*,.<>;:#$%&' + \"'\"\n",
    "    for char in dirty_chars:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faaed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(clean_text_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['clean_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 50 most common word per category and word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(df, category):\n",
    "    tokens = []\n",
    "    for series in df[df[\"category\"] == category]['tokenized']:\n",
    "        for t in series:\n",
    "            tokens.append(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = get_all_tokens(df, CATEGORY1)\n",
    "tokens2 = get_all_tokens(df, CATEGORY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stop_words):\n",
    "    '''\n",
    "    This function removes stopwords from a list of tokens.\n",
    "    Parameters:\n",
    "    - tokens = list of tokens.\n",
    "    - stop_words = list of stopwords\n",
    "    Output: \n",
    "    - tokens = list of tokens without stopwords \n",
    "    '''\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            tokens.remove(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english') #I added into a list variable to fix the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1_nostop = remove_stopwords(tokens1, stopwords_en)\n",
    "tokens2_nostop = remove_stopwords(tokens2, stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordcloud(tokens, n_words):\n",
    "    data = collections.Counter(tokens).most_common(n_words)\n",
    "    word_frequencies = {word: freq for word, freq in data}\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8c698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_wordcloud(tokens1_nostop, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wordcloud(tokens2_nostop, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram and boxplot for sentence length per category with min/max/avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1488caf",
   "metadata": {},
   "source": [
    "Attention:\n",
    "This changed after I fixed the stopwords function. \n",
    "We need to review it carefully because I don't see the tokens element here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(series):\n",
    "    sentences = []\n",
    "    for item in series:\n",
    "        for para in item.split('\\n'):\n",
    "            for sent in nltk.sent_tokenize(para):\n",
    "                sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13791562",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = get_sentences(df[df['category'] == CATEGORY1]['text']) \n",
    "sentences2 = get_sentences(df[df['category'] == CATEGORY2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8733cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_dict(sentences):\n",
    "    lengths = {}\n",
    "    longest_sentence = \"\"\n",
    "    shortest_sentence = \"Pretty sure it's smaller than this\"\n",
    "    for sentence in sentences:\n",
    "        len_sentence = len(sentence.split(' '))\n",
    "        if len_sentence > len(longest_sentence.split(' ')):\n",
    "            longest_sentence = sentence\n",
    "        if len_sentence < len(shortest_sentence.split(' ')):\n",
    "            shortest_sentence = sentence\n",
    "        if lengths.get(len_sentence):\n",
    "            lengths[len_sentence] += 1\n",
    "        else:\n",
    "            lengths[len_sentence] = 1\n",
    "    expanded = []\n",
    "    for key, count in lengths.items():\n",
    "        expanded.extend([key] * count)\n",
    "    return lengths, expanded, longest_sentence, shortest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc76e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_sentence1, expanded1, longest_sentence1, shortest_sentence1 = get_length_dict(sentences1)\n",
    "length_sentence2, expanded2, longest_sentence2, shortest_sentence2 = get_length_dict(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min Sentence Length Category 1:', min(length_sentence1))\n",
    "print('Max Sentence Length Category 1:', max(length_sentence1))\n",
    "print('Avg Sentence Length Category 1:', sum(length_sentence1) / len(length_sentence1))\n",
    "print('Min Sentence Length Category 2:', min(length_sentence2))\n",
    "print('Max Sentence Length Category 2:', max(length_sentence2))\n",
    "print('Avg Sentence Length Category 2:', sum(length_sentence2) / len(length_sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(expanded1)\n",
    "plt.title(\"Box Plot of the Sentence Length of Category 1\")\n",
    "plt.ylabel(\"Length of Sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b61141",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(expanded2)\n",
    "plt.title(\"Box Plot of the Sentence Length of Category 2\")\n",
    "plt.ylabel(\"Length of Sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(expanded1)\n",
    "plt.title(\"Histogram of the Sentence Length of Category 1\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Length of Sentence (Words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(expanded2)\n",
    "plt.title(\"Histogram of the Sentence Length of Category 2\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Length of Sentence (Words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d241ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? Total number of bi-gram occurences per category. min/max/avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25db28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_bigrams(tokens):\n",
    "    bigrams = nltk.bigrams(tokens)\n",
    "    occurences = {}\n",
    "    for bigram in bigrams:\n",
    "        if occurences.get(bigram):\n",
    "            occurences[bigram] += 1\n",
    "        else:\n",
    "            occurences[bigram] = 1\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams1 = collect_bigrams(tokens1)\n",
    "bigrams2 = collect_bigrams(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ad174",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_longest1 = collect_bigrams(longest_sentence1.split(' '))\n",
    "bigrams_longest2 = collect_bigrams(longest_sentence2.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_shortest1 = collect_bigrams(shortest_sentence1.split(' '))\n",
    "bigrams_shortest2 = collect_bigrams(shortest_sentence2.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d71612",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique bigram occurences in Category 1:\", len(bigrams1))\n",
    "print(\"\\tUnique bigrams in shortest:\\t\\t\", len(bigrams_shortest1))\n",
    "print(\"\\tUnique bigrams in longest:\\t\\t\", len(bigrams_longest1))\n",
    "print(\"Number of unique bigram occurences in Category 2:\", len(bigrams2))\n",
    "print(\"\\tUnique bigrams in shortest:\\t\\t\", len(bigrams_shortest1))\n",
    "print(\"\\tUnique bigrams in longest:\\t\\t\", len(bigrams_longest1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(category):\n",
    "    files = glob.glob(f'./data/*{category}*.json') \n",
    "    data = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            data.extend(json.load(f))\n",
    "    return data\n",
    "\n",
    "def format_property_name(uri):\n",
    "    name = uri.split('/')[-1]\n",
    "    name = name.split('#')[-1]\n",
    "    name = name.replace('_', ' ')\n",
    "    replacements = {\n",
    "        'PageID': 'Page ID',\n",
    "        'URL': 'URL',\n",
    "        'birthDate': 'Birth Date',\n",
    "        'deathDate': 'Death Date',\n",
    "        'birthPlace': 'Birth Place',\n",
    "        'deathPlace': 'Death Place',\n",
    "        'sameAs': 'Same As'\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        name = name.replace(key, value)\n",
    "    return name\n",
    "\n",
    "def count_properties(data, exclude):\n",
    "    property_counter = Counter()\n",
    "    for triple in data:\n",
    "        property_uri = triple['predicate']\n",
    "        if property_uri not in exclude:\n",
    "            simple_property_name = format_property_name(property_uri)\n",
    "            property_counter[simple_property_name] += 1\n",
    "    return property_counter\n",
    "\n",
    "def make_wordcloud(properties_counter, category):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(properties_counter)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {category}')\n",
    "    plt.show()\n",
    "#I excluded all the cases that were not relevant to describe the person. However: it does still look kinda ugly.\n",
    "exclusions = [\n",
    "    \"http://dbpedia.org/ontology/wikiPageExternalLink\",\n",
    "    \"http://dbpedia.org/ontology/wikiPageRevisionID\",\n",
    "    \"http://dbpedia.org/ontology/wikiPageWikiLink\",\n",
    "    \"http://dbpedia.org/property/wikiPageUsesTemplate\",\n",
    "    \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n",
    "    \"http://www.w3.org/2000/01/rdf-schema#label\",\n",
    "    \"http://www.w3.org/2000/01/rdf-schema#comment\",\n",
    "    \"http://www.w3.org/2000/01/rdf-schema#comment\",\n",
    "    \"http://dbpedia.org/ontology/wikiPageLength\",\n",
    "    \"http://dbpedia.org/ontology/wikiPageID\",\n",
    "    \"http://xmlns.com/foaf/0.1/isPrimaryTopicOf\",\n",
    "    \"http://www.w3.org/ns/prov#wasDerivedFrom\"\n",
    "]\n",
    "\n",
    "for category in ['Rabbi', 'Drag_queen']:\n",
    "    data = load_data(category)\n",
    "    properties = count_properties(data, exclusions)\n",
    "    make_wordcloud(properties, category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms and boxplot for number of facts per category. min/max/avg\n",
    "\n",
    "def load_and_count_facts(category):\n",
    "    files = glob.glob(f'./data/*{category}*.json')\n",
    "    fact_counts = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        unique_facts = set((fact['subject'], fact['predicate'], fact['object']) for fact in data)\n",
    "        fact_counts.append(len(unique_facts))\n",
    "    return fact_counts\n",
    "\n",
    "def visualize_data(categories):\n",
    "    for category in categories:\n",
    "        fact_counts = load_and_count_facts(category)\n",
    "        print(f\"Statistics for {category}:\")\n",
    "        print(f\"Minimum facts: {min(fact_counts)}\")\n",
    "        print(f\"Maximum facts: {max(fact_counts)}\")\n",
    "        print(f\"Average facts: {sum(fact_counts) / len(fact_counts)}\")\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(fact_counts, bins=30, alpha=0.7, label=f'{category} Facts')\n",
    "        plt.title(f'Histogram of Fact Counts for {category}')\n",
    "        plt.xlabel('Number of Facts')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.boxplot(fact_counts, vert=False)\n",
    "        plt.title(f'Boxplot of Fact Counts for {category}')\n",
    "        plt.xlabel('Number of Facts')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "categories = ['Rabbi', 'Drag_queen'] #For some reason the Rabbi's graphs look *awful*\n",
    "\n",
    "\n",
    "visualize_data(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To linearize the RDFs \n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_and_linearize(category):\n",
    "    path_pattern = f'./data/*{category}*.json'\n",
    "    linearized_data = []\n",
    "\n",
    "    for file_path in glob.glob(path_pattern):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for triple in data:\n",
    "                \n",
    "                subject = extract_and_clean(triple['subject'])\n",
    "                predicate = extract_and_clean(triple['predicate'])\n",
    "                object_ = extract_and_clean(triple['object'])\n",
    "                \n",
    "               \n",
    "                linearized_string = f\"{subject} {predicate} {object_}\"\n",
    "                linearized_data.append(linearized_string)\n",
    "\n",
    "    return linearized_data\n",
    "\n",
    "def extract_and_clean(uri):\n",
    "    \n",
    "    parts = uri.split('/')\n",
    "    last_part = parts[-1]\n",
    "    \n",
    "    if '#' in last_part:\n",
    "        last_part = last_part.split('#')[-1]\n",
    "    \n",
    "    return last_part.replace('_', ' ').replace('-', ' ')\n",
    "\n",
    "\n",
    "categories = ['Rabbi', 'Drag_queen']\n",
    "linearized_data = {}\n",
    "\n",
    "for category in categories:\n",
    "    linearized_data[category] = load_and_linearize(category)\n",
    "\n",
    "\n",
    "linearized_rabbi = linearized_data['Rabbi']\n",
    "linearized_drag_queen = linearized_data['Drag_queen']\n",
    "\n",
    "### To clean the elements of each list ### \n",
    "\n",
    "#remove repetitions\n",
    "linearized_rabbi_rdf = list(set(linearized_rabbi))\n",
    "linearized_drag_queen_rdf = list(set(linearized_drag_queen))\n",
    "\n",
    "#remove > symbol\n",
    "\n",
    "linearized_rabbi_rdf = [re.sub('>', '', rabbi) for rabbi in linearized_rabbi_rdf]\n",
    "linearized_drag_queen_rdf = [re.sub('>', '', dg) for dg in linearized_drag_queen_rdf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KMeans clustering (n_clusters=2). Cluster based on text (?) and, separately, on facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c94da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute supervised and unsupervised metrics and visualizations to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that process a document and return the set of named entities(NEs) (Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for each output (avg/min/max number of NEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b316f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for each output (avg/min/max number of words (unclear use of word 'word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the above 2 statistics (per category, per package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes a single document and performs the following 5 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Returns # of spans that both packages agree on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Returns # of partial agreements on spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Returns for each package # of spans that one package predicted as a NE and the other did not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Returns for all spans with full and partial agreement, agreement about NE types (aka Person, Location, Organisation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Visualize the above statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of NEs predicted by each package, how many of them can be found in the knowledge graph (KG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each package, the ratio of predicted NEs that can be found in the KG of a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
